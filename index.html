<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>OSCILLATION INVERSION: UNDERSTANDING THE STRUCTURE OF LARGE FLOW MODELS THROUGH THE LENS OF INVERSION METHODS</title>
  <link rel="stylesheet" href="./style.css"> <!-- Link your stylesheet if needed -->
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0 auto;
      padding: 20px;
      max-width: 1000px;
    }
    h1, h2, h3 {
      text-align: center;
    }
    .content {
      margin: 20px 0;
    }
    .content img {
      max-width: 100%;
      display: block;
      margin: 0 auto;
    }
    #authors {
      text-align: center;
      font-size: 18px;
    }
    #authors a {
      color: blue;
      text-decoration: none;
    }
    #authors sup {
      font-size: 12px;
    }
    .links {
      text-align: center;
      margin: 20px 0;
    }
    .links a {
      margin: 0 15px;
      font-size: 18px;
      text-decoration: none;
      color: blue;
    }
    .summary-img {
      border: 2px solid orange;
      display: block;
      margin: 10px auto;
    }
    ul {
      margin: 20px 0;
      padding-left: 20px;
    }
    ul li {
      margin-bottom: 10px;
    }
    code {
      background-color: #f4f4f4;
      padding: 10px;
      display: block;
      margin: 10px 0;
      font-size: 14px;
    }
  </style>
</head>

<body>

  <!-- Title Section -->
  <div class="content">
    <h1><strong>OSCILLATION INVERSION: UNDERSTANDING THE STRUCTURE OF LARGE FLOW MODELS THROUGH THE LENS OF INVERSION METHODS</strong></h1>
  </div>

  <!-- Authors Section -->
  <div id="authors">
    <a href="#">Yan Zheng<sup>1,2</sup></a>, 
    <a href="#">Zhenxiao Liang<sup>2</sup></a>, 
    <a href="#">Xiaoyan Cong<sup>2</sup></a>, 
    <a href="#">Yuehao Wang<sup>1</sup></a>, 
    <a href="#">Peihao Wang<sup>1</sup></a>, 
    <a href="#">Lanqing Guo<sup>2</sup></a><br>
    <a href="#">Lanqing Guo<sup>2</sup></a><br>
    <span><sup>1</sup>The University of Texas at Austin, <sup>2</sup>Google</span>
  </div>

  <!-- Links Section -->
  <div class="links">
    <a href="./paper.pdf" target="_blank">[Paper]</a>
    <a href="https://arxiv.org" target="_blank">[arXiv]</a>
    <a href="https://github.com/username/repository" target="_blank">[Code]</a>
  </div>

  <!-- Main Image -->
  <div class="content">
    <img src="./data/main.png" alt="Main Teaser" class="teaser-gif">
    <p><b>Rectified flows for image inversion and editing</b>: Our method efficiently inverts reference style images without needing text descriptions, enabling semantic image editing based on prompts.</p>
  </div>

  <!-- Abstract Section -->
  <div class="content">
    <h2>Abstract</h2>
    <p>Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses the inversion and editing of real images using stochastic rectified flow models. We propose a novel inversion method that provides state-of-the-art performance in zero-shot inversion and editing, outperforming prior works with qualitative and quantitative results.</p>
  </div>

  <!-- Contributions Section -->
  <div class="content">
    <h2>Contributions</h2>
    <ul>
      <li>An efficient inversion method for rectified flow models, requiring no additional training or complex attention processors.</li>
      <li>A novel vector field for RF inversion that balances image fidelity with alignment to the true distribution of clean images.</li>
      <li>Extensive evaluations across three benchmarks (LSUN-Bedroom, LSUN-Church, SFHQ) on stroke-to-image synthesis and image editing, along with user preference evaluations.</li>
    </ul>
  </div>

  <!-- Graphical Model Section -->
  <div class="content">
    <h2>Graphical Model</h2>
    <p>A comparison of DDIM inversion and RF inversion. RF inversion significantly reduces deviation between the original image and the reconstructed latent space, improving faithfulness.</p>
    <img src="./data/graph.png" alt="Graphical Model" class="summary-img">
  </div>

  <!-- Stylization Results Section -->
  <div class="content">
    <h2>Stylization Results</h2>
    <p><b>Stylization with a single reference image:</b> Our method generates stylized images consistent with the reference style and text prompts.</p>
    <img src="./data/stylization.png" alt="Stylization Results" class="summary-img">
  </div>

  <!-- Cartoonization Results Section -->
  <div class="content">
    <h2>Cartoonization Results</h2>
    <img src="./data/cartoonization.png" alt="Cartoonization Results" class="summary-img">
    <h3>Cartoonization with prompt-based facial expressions in "Disney 3D cartoon style".</h3>
  </div>

  <!-- Stroke-to-Image Generation Section -->
  <div class="content">
    <h2>Stroke-to-Image Generation Results</h2>
    <img src="./data/stroke2image-bedroom.png" alt="Stroke to Image - Bedroom" class="summary-img">
    <h3>LSUN-Bedroom dataset: Comparison with state-of-the-art methods.</h3>
  </div>

  <!-- Semantic Image Editing Section -->
  <div class="content">
    <h2>Semantic Image Editing Results</h2>
    <img src="./data/glasses.png" alt="Adding Glasses" class="summary-img">
    <h3>Adding glasses using the prompt "wearing glasses".</h3>
    <img src="./data/age.png" alt="Age Editing" class="summary-img">
    <h3>Age editing: Our method regulates the extent of age editing.</h3>
  </div>

  <!-- Text-to-Image Generation Section -->
  <div class="content">
    <h2>Text-to-Image Generation</h2>
    <p>Qualitative results on text-to-image generation using rectified stochastic differential equations (SDE) across different discretization steps.</p>
    <img src="./data/t2i-gen.png" alt="Text-to-Image Generation" class="summary-img">
  </div>

  <!-- BibTex Section -->
  <div class="content">
    <h2>BibTex</h2>
    <code>
      @article{rout2024rfinversion,<br>
      &nbsp;&nbsp;title={Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations},<br>
      &nbsp;&nbsp;author={Rout, L and Chen, Y and Ruiz, N and Caramanis, C and Shakkottai, S and Chu, W},<br>
      &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2410.10792},<br>
      &nbsp;&nbsp;year={2024}<br>
      }
    </code>
  </div>

  <!-- Acknowledgements Section -->
  <div class="content">
    <p><strong>Acknowledgements:</strong> This research is supported by NSF Grant 2019844, a Google research award, and UT Austin ML Lab.</p>
  </div>

</body>
</html>
